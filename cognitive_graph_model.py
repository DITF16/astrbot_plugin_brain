from typing import List, Tuple, Dict

import torch
import torch.nn as nn
import math

# === é€šé“å®šä¹‰å¸¸é‡ ===
CHANNEL_IS_A = 0       # å®šä¹‰: A æ˜¯ B
CHANNEL_HAS_PROP = 1   # å±æ€§: A æœ‰ B
CHANNEL_CAUSES = 2     # å› æœ: A å¯¼è‡´ B
CHANNEL_ASSOCIATED = 3 # ç›´è§‰: A è”æƒ³ B (ä¿ç•™æ—§æœ‰é€»è¾‘)
CHANNEL_REPRESENTS = 4 # è¡¨å¾: A å¯¹åº” B (å¤šæ¨¡æ€é”šç‚¹)
NUM_CHANNELS = 5

class CognitiveGraphModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=256):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_channels = NUM_CHANNELS

        # 1. é™æ€åŸºå› ï¼šè¯å‘é‡ (L0 æ„ŸçŸ¥å±‚)
        self.embeddings = nn.Embedding(vocab_size, embed_dim)

        # 2. åŠ¨æ€çªè§¦ï¼šä¸‰ç»´å›¾è¿æ¥å¼ é‡ [Channels, N, N]
        initial_tensor = torch.zeros(NUM_CHANNELS, vocab_size, vocab_size)
        
        # åˆå§‹åŒ–ç›´è§‰é€šé“
        initial_tensor[CHANNEL_ASSOCIATED] = torch.eye(vocab_size) + torch.randn(vocab_size, vocab_size) * 0.01
        
        self.synapse_tensor = nn.Parameter(initial_tensor)

        # 3. åŠ¨æ€ç»éªŒï¼šè¯é¢‘ç»Ÿè®¡ (è®°å¿† Buffer)
        self.register_buffer("word_counts", torch.ones(vocab_size))
        self.register_buffer("total_experience", torch.tensor(float(vocab_size)))
        
        # 4. æƒ…ç»ªè°ƒèŠ‚å™¨
        self.register_buffer("mood_bias", torch.zeros(vocab_size)) 

    def get_attention_weights(self, input_indices):
        counts = self.word_counts[input_indices]
        total = self.total_experience
        weights = torch.log(total + 1) / (torch.log(counts + 1) + 1e-6)
        weights = torch.clamp(weights * 0.5, min=0.1, max=3.0)
        return weights

    def learn_from_input(self, input_indices):
        flat_indices = input_indices.view(-1)
        for idx in flat_indices:
            self.word_counts[idx] += 1
            self.total_experience += 1

    def reinforce(self, indices, reward_sign):
        if len(indices) < 2: return
        with torch.no_grad():
            for i in range(len(indices) - 1):
                u, v = indices[i], indices[i+1]
                self.synapse_tensor[CHANNEL_ASSOCIATED, u, v] += (reward_sign * 0.5)
                if reward_sign > 0:
                    self.mood_bias[v] += 0.1
                else:
                    self.mood_bias[v] -= 0.1

    def process_sleep_cycle(self, pruning_threshold=0.005):
        """
        ç¡çœ æœºåˆ¶ï¼šä¿®å¤è¿‡åº¦æ¸…ç†
        """
        with torch.no_grad():
            total_pruned = 0
            total_count = 0

            # === 1. å‹åŠ›æ£€æµ‹ ===
            target_capacity = float(self.vocab_size) * 50.0
            current_energy = self.synapse_tensor.abs().sum().item()

            if current_energy > target_capacity:
                pressure_ratio = target_capacity / current_energy
                global_decay = max(pressure_ratio, 0.95)
            else:
                global_decay = 1.0

            num_channels = self.synapse_tensor.shape[0]

            for c in range(num_channels):
                channel_data = self.synapse_tensor.data[c]

                # --- æ™¶ä½“åŒ–ä¿æŠ¤ (é™ä½ä¿æŠ¤é˜ˆå€¼) ---
                weights_abs = channel_data.abs()
                shield = torch.sigmoid((weights_abs - 0.2) * 8.0)
                final_decay_c = global_decay * (1.0 - shield) + shield
                channel_data.mul_(final_decay_c)
                del weights_abs, shield, final_decay_c

                # --- å¯¹æ¯”åº¦å¢å¼º (æ›´æ¸©å’Œ) ---
                signs = torch.sign(channel_data)
                channel_data.abs_()
                channel_data.pow_(1.02)  # ä»1.1é™åˆ°1.02ï¼Œéå¸¸æ¸©å’Œ
                channel_data.mul_(signs)
                del signs

                # --- åŠ¨æ€èƒ½é‡å®ˆæ’ ---
                row_sums = channel_data.abs().sum(dim=1, keepdim=True) + 1e-6
                row_maxs = channel_data.abs().max(dim=1, keepdim=True)[0]
                sparsity = row_maxs / row_sums
                gate = torch.tanh(sparsity * 5.0)

                counts = self.word_counts
                base_limit = 30.0
                freq_bonus = torch.log1p(counts).view(-1, 1).to(channel_data.device) * 3.0

                dynamic_limits = base_limit + freq_bonus * gate
                scaling_factor = (dynamic_limits / row_sums).clamp(max=1.0)
                channel_data.mul_(scaling_factor)
                del row_sums, row_maxs, sparsity, gate, scaling_factor

                # --- å‰ªæ (åªæ¸…ç†çœŸæ­£çš„å™ªå£°) ---
                mask = channel_data.abs() < pruning_threshold
                total_pruned += mask.sum().item()
                total_count += mask.numel()
                channel_data.masked_fill_(mask, 0.0)
                del mask

            self.mood_bias.mul_(0.95)

            if self.synapse_tensor.is_cuda:
                torch.cuda.empty_cache()

            return total_pruned, total_count, global_decay

    def forward(self, input_indices, steps=3, channel_weights=None):
        batch_size, seq_len = input_indices.shape
        device = input_indices.device

        if channel_weights is None:
            channel_weights = torch.tensor([0.3, 0.2, 0.3, 0.2, 0.0], device=device)

        channel_weights = channel_weights / channel_weights.sum()
        mixed_synapse = torch.einsum('c,cij->ij', channel_weights, self.synapse_tensor)
        attn_weights = self.get_attention_weights(input_indices)

        current_thought = torch.zeros(
            batch_size, self.vocab_size, device=device
        )
        current_thought.scatter_add_(1, input_indices, attn_weights)

        for _ in range(steps):
            current_thought = torch.matmul(current_thought, mixed_synapse)
            current_thought = torch.relu(current_thought - 0.1)
            current_thought += self.mood_bias * 0.05

        return current_thought

    def _learn_associations(self, input_indices, learning_rate=0.1):
        """
        è‡ªåŠ¨å­¦ä¹ ï¼šç›¸é‚»çš„è¯å»ºç«‹å…³è”è¿æ¥
        """
        with torch.no_grad():
            for batch in input_indices:
                indices = batch.tolist()
                for i in range(len(indices) - 1):
                    u, v = indices[i], indices[i + 1]

                    # è·³è¿‡ç‰¹æ®Štoken (PAD=0, UNK=1 ç­‰)
                    if u <= 1 or v <= 1:
                        continue

                    # åŒå‘å¢å¼ºå…³è”é€šé“
                    current_weight = self.synapse_tensor[CHANNEL_ASSOCIATED, u, v].item()

                    # ä½¿ç”¨é€’å‡å­¦ä¹ ç‡ï¼šå·²æœ‰å¼ºè¿æ¥æ—¶å­¦å¾—æ…¢ï¼Œå¼±è¿æ¥å­¦å¾—å¿«
                    effective_lr = learning_rate / (1.0 + abs(current_weight))

                    self.synapse_tensor.data[CHANNEL_ASSOCIATED, u, v] += effective_lr
                    self.synapse_tensor.data[CHANNEL_ASSOCIATED, v, u] += effective_lr * 0.5  # åå‘å¼±ä¸€ç‚¹

    def generate_reply(self, input_indices, max_len=20, channel_weights=None):
        self.eval()
        device = input_indices.device

        with torch.no_grad():
            thought_energy = self(input_indices, steps=1, channel_weights=channel_weights)

        if channel_weights is None:
             channel_weights = torch.tensor([0.3, 0.2, 0.3, 0.2, 0.0], device=device)
        channel_weights = channel_weights / channel_weights.sum()
        mixed_synapse = torch.einsum('c,cij->ij', channel_weights, self.synapse_tensor)

        visited = set(input_indices[0].tolist())
        visited.add(0) 
        visited.add(1)

        start_energy = thought_energy[0].clone()
        for v in visited:
            start_energy[v] = -float("inf")

        max_val, max_idx = torch.max(start_energy, dim=0)
        
        current_idx = -1
        if max_val > 0.5: 
            current_idx = max_idx.item()
        else:
            if max_val == -float("inf"):
                 candidates = torch.ones_like(start_energy)
                 candidates[0] = 0
                 candidates[1] = 0
                 probs = candidates / candidates.sum()
            else:
                probs = torch.softmax(start_energy * 5.0, dim=0)
            
            current_idx = torch.multinomial(probs, 1).item()

        if current_idx in [0, 1]:
             return []

        reply_indices = [current_idx]
        visited.add(current_idx)
        
        for _ in range(max_len):
            next_step_weights = mixed_synapse[current_idx].clone()

            strongest_conn = torch.max(next_step_weights)
            strongest_idx = torch.argmax(next_step_weights).item()
            
            if strongest_conn > 0.8 and strongest_idx not in visited and strongest_idx > 1:
                 next_idx = strongest_idx
            else:
                for v in visited:
                    next_step_weights[v] = -float("inf")
                
                next_step_weights[0] = -float("inf")
                next_step_weights[1] = -float("inf")

                guidance = thought_energy[0] * 0.5
                combined_weights = next_step_weights + guidance

                if torch.max(combined_weights) == -float("inf"):
                    break

                next_probs = torch.softmax(combined_weights * 3.0, dim=0)
                next_idx = torch.multinomial(next_probs, 1).item()

            reply_indices.append(next_idx)
            visited.add(next_idx)
            current_idx = next_idx

        return reply_indices

    # =========================================================
    # ğŸ”— å› æœæ¨ç†æ”¯æŒæ–¹æ³•
    # =========================================================

    def get_direct_effects(self, concept_idx: int, top_k: int = 10,
                           min_strength: float = 0.1) -> List[Tuple[int, float]]:
        """
        è·å–æ¦‚å¿µçš„ç›´æ¥å› æœåæœ

        Args:
            concept_idx: æ¦‚å¿µç´¢å¼•
            top_k: è¿”å›å‰kä¸ªç»“æœ
            min_strength: æœ€å°å¼ºåº¦é˜ˆå€¼

        Returns:
            [(effect_idx, strength), ...]
        """
        if concept_idx <= 1:
            return []

        effects = self.synapse_tensor[CHANNEL_CAUSES, concept_idx]

        # è¿‡æ»¤ä½äºé˜ˆå€¼çš„
        mask = effects > min_strength
        if not mask.any():
            return []

        # è·å– top-k
        values, indices = torch.topk(effects, min(top_k + 2, len(effects)))

        results = []
        for val, idx in zip(values.tolist(), indices.tolist()):
            if val < min_strength:
                break
            if idx <= 1 or idx == concept_idx:
                continue
            results.append((idx, val))

        return results[:top_k]

    def get_direct_causes(self, concept_idx: int, top_k: int = 10,
                          min_strength: float = 0.1) -> List[Tuple[int, float]]:
        """
        è·å–æ¦‚å¿µçš„ç›´æ¥åŸå›  (é€†å‘å› æœ)

        Args:
            concept_idx: æ¦‚å¿µç´¢å¼•
            top_k: è¿”å›å‰kä¸ªç»“æœ
            min_strength: æœ€å°å¼ºåº¦é˜ˆå€¼

        Returns:
            [(cause_idx, strength), ...]
        """
        if concept_idx <= 1:
            return []

        # è½¬ç½®å› æœçŸ©é˜µçš„å¯¹åº”åˆ—
        causes = self.synapse_tensor[CHANNEL_CAUSES, :, concept_idx]

        mask = causes > min_strength
        if not mask.any():
            return []

        values, indices = torch.topk(causes, min(top_k + 2, len(causes)))

        results = []
        for val, idx in zip(values.tolist(), indices.tolist()):
            if val < min_strength:
                break
            if idx <= 1 or idx == concept_idx:
                continue
            results.append((idx, val))

        return results[:top_k]

    def strengthen_causal_link(self, cause_idx: int, effect_idx: int,
                               delta: float = 0.5) -> None:
        """
        å¼ºåŒ–å› æœè¿æ¥

        Args:
            cause_idx: åŸå› æ¦‚å¿µç´¢å¼•
            effect_idx: ç»“æœæ¦‚å¿µç´¢å¼•
            delta: å¼ºåŒ–é‡
        """
        if cause_idx <= 1 or effect_idx <= 1:
            return

        with torch.no_grad():
            current = self.synapse_tensor[CHANNEL_CAUSES, cause_idx, effect_idx]
            # ä½¿ç”¨è¡°å‡å¢é‡é¿å…çˆ†ç‚¸
            effective_delta = delta / (1.0 + current.abs().item())
            self.synapse_tensor[CHANNEL_CAUSES, cause_idx, effect_idx] += effective_delta

            # é’³ä½
            self.synapse_tensor[CHANNEL_CAUSES, cause_idx, effect_idx].clamp_(-10.0, 10.0)

    def get_causal_subgraph(self, center_idx: int, radius: int = 2) -> Dict:
        """
        è·å–ä»¥æŸæ¦‚å¿µä¸ºä¸­å¿ƒçš„å› æœå­å›¾

        Args:
            center_idx: ä¸­å¿ƒæ¦‚å¿µç´¢å¼•
            radius: æœç´¢åŠå¾„

        Returns:
            {
                "nodes": [(idx, word), ...],
                "edges": [(source_idx, target_idx, strength), ...]
            }
        """
        if center_idx <= 1:
            return {"nodes": [], "edges": []}

        nodes = set()
        edges = []

        # BFS
        queue = [(center_idx, 0)]
        visited = set()

        while queue:
            current, depth = queue.pop(0)

            if current in visited:
                continue
            visited.add(current)
            nodes.add(current)

            if depth >= radius:
                continue

            # æ­£å‘: å½“å‰èŠ‚ç‚¹çš„åæœ
            effects = self.synapse_tensor[CHANNEL_CAUSES, current]
            for eff_idx in (effects > 0.1).nonzero(as_tuple=True)[0].tolist():
                if eff_idx > 1:
                    strength = effects[eff_idx].item()
                    edges.append((current, eff_idx, strength))
                    if eff_idx not in visited:
                        queue.append((eff_idx, depth + 1))

            # é€†å‘: å¯¼è‡´å½“å‰èŠ‚ç‚¹çš„åŸå› 
            causes = self.synapse_tensor[CHANNEL_CAUSES, :, current]
            for cause_idx in (causes > 0.1).nonzero(as_tuple=True)[0].tolist():
                if cause_idx > 1:
                    strength = causes[cause_idx].item()
                    edges.append((cause_idx, current, strength))
                    if cause_idx not in visited:
                        queue.append((cause_idx, depth + 1))

        return {
            "nodes": list(nodes),
            "edges": edges
        }
