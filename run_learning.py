import torch
from cognitive_graph_model import CognitiveGraphModel  # å‡è®¾ä¹‹å‰çš„ç±»ä¿å­˜åœ¨è¿™
from cognitive_trainer import HebbianTrainer

# --- 1. å‡†å¤‡æç®€çš„ä¸–ç•ŒçŸ¥è¯† (Micro World) ---
# æ‰‹åŠ¨æ„å»ºä¸€ä¸ªå¾®å‹è¯è¡¨
vocab = [
    "<PAD>",
    "å¥³ç‹",
    "æ˜¯",
    "å¥³æ€§",
    "è‹¹æœ",
    "æ°´æœ",
    "æ¯’",
    "å¥½åƒ",
    "çº¢è‰²çš„",
    "å–œæ¬¢",
    "åƒ",
    "æƒåŠ›",
    "å®«æ®¿",
]
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for i, w in enumerate(vocab)}

# å‡†å¤‡å‡ æ¡ç®€å•çš„è®­ç»ƒè¯­æ–™
corpus = [
    ["å¥³ç‹", "æ˜¯", "å¥³æ€§"],
    ["å¥³ç‹", "å–œæ¬¢", "æƒåŠ›"],
    [
        "å¥³ç‹",
        "ä½",
        "åœ¨",
        "å®«æ®¿",
    ],  # æ³¨æ„ï¼š'ä½'å’Œ'åœ¨'ä¸åœ¨è¯è¡¨é‡Œï¼Œæ¼”ç¤ºæ—¶ä¼šæŠ¥é”™ï¼Œä¸‹é¢éœ€å¤„ç†
    ["è‹¹æœ", "æ˜¯", "æ°´æœ"],
    ["è‹¹æœ", "æ˜¯", "çº¢è‰²çš„"],
    ["å¥³ç‹", "åƒ", "è‹¹æœ"],
    ["æ¯’", "è‹¹æœ", "æ˜¯", "çº¢è‰²çš„"],  # åˆ¶é€ ä¸€ç‚¹æ··æ·†é€»è¾‘
]


# é¢„å¤„ç†ï¼šæŠŠä¸åœ¨è¯è¡¨é‡Œçš„è¯è¿‡æ»¤æ‰ (æ¨¡æ‹Ÿå¬ä¸æ‡‚)
def tokenize(text_list):
    return [word2idx[w] for w in text_list if w in word2idx]


clean_corpus = [tokenize(s) for s in corpus]

# --- 2. åˆå§‹åŒ–å¤§è„‘ ---
print("ğŸ§  åˆå§‹åŒ–å¤§è„‘...")
model = CognitiveGraphModel(vocab_size=len(vocab), embed_dim=16)
trainer = HebbianTrainer(model, learning_rate=0.5)  # å­¦ä¹ ç‡è®¾å¤§ç‚¹ï¼Œæ•ˆæœç«‹ç«¿è§å½±

# --- 3. å¼€å§‹å­¦ä¹  (Training Loop) ---
print("\nğŸ“š å¼€å§‹å­¦ä¹ é˜¶æ®µ...")
for epoch in range(5):  # è¯»5éä¹¦
    print(f"--- Epoch {epoch+1} ---")
    for sentence in clean_corpus:
        if not sentence:
            continue
        avg_weight = trainer.train_step(sentence)

    # æ‰“å°ä¸€ç‚¹å†…éƒ¨çŠ¶æ€çœ‹çœ‹
    # çœ‹çœ‹"å¥³ç‹"ç°åœ¨çš„ç»éªŒå€¼
    q_idx = word2idx["å¥³ç‹"]
    count = model.word_counts[q_idx].item()
    print(f"   [çŠ¶æ€] 'å¥³ç‹' è¢«æ¿€æ´»æ¬¡æ•°: {int(count)}")

# --- 4. æ£€éªŒæˆæœ (Inference) ---
print("\nâœ¨ å­¦ä¹ ç»“æŸï¼Œå¼€å§‹æµ‹è¯•è”æƒ³èƒ½åŠ›...")


def chat(start_word):
    if start_word not in word2idx:
        print("??? æˆ‘æ²¡å­¦è¿‡è¿™ä¸ªè¯ã€‚")
        return

    start_idx = word2idx[start_word]
    input_tensor = torch.tensor([[start_idx]])  # [1, 1]

    print(f"\nQ: è¯´è¯´å…³äº'{start_word}'çš„äº‹ï¼Ÿ")

    # ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å†™çš„ generate_reply
    reply_ids = model.generate_reply(input_tensor, max_len=5)

    # è§£ç å›æ–‡å­—
    reply_words = [idx2word[idx] for idx in reply_ids]
    print(f"A: {' -> '.join(reply_words)}")


# æµ‹è¯• 1: é—®å¥³ç‹
chat("å¥³ç‹")
# é¢„æœŸé€»è¾‘é“¾ï¼šå¥³ç‹ -> æƒåŠ› / å–œæ¬¢ / è‹¹æœ

# æµ‹è¯• 2: é—®è‹¹æœ
chat("è‹¹æœ")
# é¢„æœŸé€»è¾‘é“¾ï¼šè‹¹æœ -> çº¢è‰²çš„ / æ°´æœ / åƒ
