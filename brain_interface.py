import torch
import os
import jieba  # éœ€è¦ pip install jieba
from cognitive_graph_model import CognitiveGraphModel
from cognitive_trainer import HebbianTrainer


class BrainInterface:
    def __init__(self, model_path="my_brain.pth", vocab_limit=5000):
        self.model_path = model_path
        self.vocab_limit = vocab_limit
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 1. è¯è¡¨ç®¡ç† (Mapping)
        # æˆ‘ä»¬é¢„ç•™ä¸€ä¸ªå¤§çš„ç©ºé—´ (vocab_limit)ï¼Œå°±åƒå©´å„¿å¤§è„‘é¢„å…ˆé•¿å¥½äº†ç¥ç»å…ƒ
        self.word2idx = {"<PAD>": 0, "<UNK>": 1}
        self.idx2word = {0: "<PAD>", 1: "<UNK>"}
        self.next_idx = 2

        # 2. åˆå§‹åŒ–æ¨¡å‹
        self.model = CognitiveGraphModel(vocab_size=vocab_limit, embed_dim=64).to(self.device)
        self.trainer = HebbianTrainer(self.model, learning_rate=0.1)

        # 3. å°è¯•åŠ è½½å­˜æ¡£
        self.load_brain()

    def load_brain(self):
        if os.path.exists(self.model_path):
            print(f"ğŸ§  æ­£åœ¨å”¤é†’å¤§è„‘: {self.model_path} ...")
            checkpoint = torch.load(self.model_path, map_location=self.device)

            # æ¢å¤æ¨¡å‹å‚æ•°
            self.model.load_state_dict(checkpoint['model_state'])

            # æ¢å¤è¯è¡¨ (è¿™æ˜¯å…³é”®ï¼æ²¡æœ‰è¯è¡¨ï¼Œæ¨¡å‹å°±æ˜¯åºŸé“)
            vocab_data = checkpoint['vocab']
            self.word2idx = vocab_data['word2idx']
            self.idx2word = {int(k): v for k, v in vocab_data['idx2word'].items()}  # JSON keyæ˜¯strï¼Œéœ€è½¬int
            self.next_idx = vocab_data['next_idx']
            print(f"âœ… å”¤é†’æˆåŠŸã€‚å½“å‰è¯æ±‡é‡: {self.next_idx}/{self.vocab_limit}")
        else:
            print("âœ¨ åˆ›å»ºäº†ä¸€ä¸ªå…¨æ–°çš„å¤§è„‘ã€‚")

    def save_brain(self):
        print("ğŸ’¾ æ­£åœ¨è¿›å…¥ç¡çœ  (ä¿å­˜è®°å¿†)...")
        state = {
            'model_state': self.model.state_dict(),
            'vocab': {
                'word2idx': self.word2idx,
                'idx2word': self.idx2word,
                'next_idx': self.next_idx
            }
        }
        torch.save(state, self.model_path)
        print("âœ… è®°å¿†å·²å›ºåŒ–ã€‚")

    def _encode(self, text):
        """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼ŒåŠ¨æ€å­¦ä¹ æ–°è¯"""
        # ä½¿ç”¨ jieba åˆ†è¯ (å¤„ç†ä¸­æ–‡)
        words = list(jieba.cut(text))
        indices = []

        for w in words:
            if w.strip() == "": continue  # è·³è¿‡ç©ºæ ¼

            if w in self.word2idx:
                indices.append(self.word2idx[w])
            else:
                # é‡åˆ°æ–°è¯ï¼šå¦‚æœæ˜¯æ–°æ¦‚å¿µä¸”å¤§è„‘è¿˜æœ‰ç©ºé—´ï¼Œå°±æ³¨å†Œå®ƒ
                if self.next_idx < self.vocab_limit:
                    new_id = self.next_idx
                    self.word2idx[w] = new_id
                    self.idx2word[new_id] = w
                    indices.append(new_id)
                    self.next_idx += 1
                else:
                    # å¤§è„‘æ»¡äº†ï¼Œè§†ä¸ºæœªçŸ¥ (æˆ–è€…ä½ å¯ä»¥å®ç°æ·˜æ±°æœºåˆ¶)
                    indices.append(self.word2idx["<UNK>"])

        return indices

    def learn(self, text):
        """[è¾“å…¥æ¥å£] å¬åˆ°ä¸€å¥è¯ -> å­¦ä¹ """
        indices = self._encode(text)
        if len(indices) < 2: return 0.0  # å¤ªçŸ­æ²¡æ³•è”æƒ³

        # è°ƒç”¨æˆ‘ä»¬ä¹‹å‰çš„è®­ç»ƒå™¨
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸åš batch å¤„ç†ï¼Œæ¥ä¸€å¥å­¦ä¸€å¥ (Online Learning)
        loss = self.trainer.train_step(indices)
        return loss

    def reply(self, text):
        """[è¾“å‡ºæ¥å£] å¬åˆ°ä¸€å¥è¯ -> è”æƒ³å›å¤"""
        indices = self._encode(text)
        if not indices: return "..."

        # æŠŠ list è½¬ tensor
        input_tensor = torch.tensor([indices], device=self.device)

        # è°ƒç”¨ç”Ÿæˆ
        out_indices = self.model.generate_reply(input_tensor)

        # è§£ç å›æ–‡å­—
        reply_words = [self.idx2word.get(idx, "") for idx in out_indices]
        return "".join(reply_words)
